{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn 13 | Naive Bayes Classification | Belajar Machine Learning Dasar\n",
    "\n",
    "Referensi: https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "## Bayes' Theorem\n",
    "\n",
    "Disini kita akan mengenal \"Bayes' Theorem\". Bayes' theorem menawarkan suatu formula untuk menghitung nilai probability dari suatu event dengan memanfaatkan pengetahuan sebelumnya dari kondisi terkait; atau sering kali dikenal dengan istilah conditional probability.\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n",
    "\n",
    "+ Pada formula diatas merupakan formula probabilitas dari kemunculan event A bila diketahui event B muncul \"P(B|A)\".\n",
    "+ Proses kalkulasinya bisa dilakukan dengan cara mencari tahu terlebih dahulu probabiltas dari kemunculan event B bila diketahui event A muncul \"P(B|A)\".\n",
    "+ Lalu dikalikan dengan nilai probabilitas kemunculan event A \"P(A)\".\n",
    "+ Lalu dibagi dengan nilai probabilitas kemunculan event B \"P(B)\"\n",
    "\n",
    "$P(y|X) = \\frac{P(X|y) \\times P(y)}{P(X)}$\n",
    "\n",
    "Diatas merupakan formula yang sama pada formula sebelumnya, namun kali ini dalam konteks klasifikasi.\n",
    "\n",
    "+ Pada formula diatas merupakan formula probabilitas dari kemunculan nilai target label y bila diketahui kemunculan sekumpulan nilai feature X \"P(y|X)\".\n",
    "+ Proses kalkulasinya bisa dilakukan dengan cara mencari tahu probabilitas dari kemunculan sekumpulan nilai feature X bila diketahui kemunculan nilai target label y \"P(X|y)\".\n",
    "+ Lalu dikalikan dengan nilai probabilitas dari kemunculan nilai target label y \"P(y)\".\n",
    "+ Lalu dibagi dengan probabilitas dari kemunculan sekumpulan nilai feature X \"P(X)\".\n",
    "\n",
    "$Posterior = \\frac{ Likelihood \\times Prior}{Evidence}$\n",
    "\n",
    "Jika kita ingin mengadopsi terminologi Naive Bayes, maka formulanya bisa direpresentasikan seperti pada rumus diatas. Dimana nilai $Posterior$ dicari dengan cara mengkalikan nilai $Likelihood$ dengan nilai $Prior$ lalu dibagi dengan nilai $Evidence$.\n",
    "\n",
    "## Pengenalan Naive Bayes Classification\n",
    "\n",
    "##### Studi Kasus 1\n",
    "\n",
    "Berikut adalah studi kasus yang harapannya dapat membantu kita untuk lebih memahami **Naive Bayes** dengan lebih mudah. Semisal disini terdapat sebuah warung yang menawarkan 3 menu yaitu siomay, bakso, dan lumpia, warung ini juga hanya memiliki 2 pelanggan saja yaitu Asep dan Joko. Untuk setiap pelanggan sudah didata probabilitas pesanan mereka untuk setiap menu yang ditawarkan. Gambar dibawah ini merupakan ilustrasinya.\n",
    "\n",
    "![](./images/asep_joko_snack.png)\n",
    "\n",
    "Dari gambar tersebut kita bisa menarik beberapa informasi, yaitu :\n",
    "\n",
    "**Asep**\n",
    "\n",
    "+ Probability untuk Asep memesan siomay adalah 0.1.\n",
    "+ Probability untuk Asep memesan bakso adalah 0.8.\n",
    "+ Probability untuk Asep memesan lumpia adalah 0.1.\n",
    "\n",
    "**Joko**\n",
    "\n",
    "+ Probability untuk Joko memesan siomay adalah 0.5.\n",
    "+ Probability untuk Joko memesan bakso adalah 0.2.\n",
    "+ Probability untuk Joko memesan lumpia adalah 0.3.\n",
    "\n",
    "Untuk konteks pada kasus tersebut, Asep dan Joko akan berperan sebagai target label atau class. Disini kita akan menerapkan klasifikasi sederhana dengan Naive Bayes untuk memprediksi siapa pelanggan yang memesan bila diketahui kombinasi pesanannya.\n",
    "\n",
    "Berikut adalah misi atau hal yang harus kita lakukan.\n",
    "\n",
    "**Misi** : Lakukan prediksi siapa pelanggan yang melakukan pemesanan dengan diketahui pesanannya adalah **lumpia** dan **bakso**.\n",
    "\n",
    "##### Prior Probability: $P(y)$\n",
    "\n",
    "Terminologi pertama yang akan kita pelajari adalah Prior Probablity. Prior adalah suatu nilai probabilitas dari kemunculan suatu nilai target label tertentu tanpa memperhatikan nilai feature nya, nilai ini tentunya akan bergantung pada dataset yang kita miliki. Untuk kasus kita kali ini, kita bisa asumsikan bahwa peluang untuk suatu pemesanan yang dilakukan oleh Asep atau Joko adalah berimbang. Oleh karenanya, prior dari kasus kita kali ini bisa diekspresikan sebagai berikut.\n",
    "\n",
    "+ Referensi: https://en.wikipedia.org/wiki/Prior_probability\n",
    "+ $P(Asep) = 0.5$\n",
    "+ $P(Joko) = 0.5$\n",
    "\n",
    "Dari pernyataan diatas kita bisa mengetahui bahwa probabilitas pemesanan yang dilakukan oleh Asep adalah 0.5 atau 50% dan probabilitas pemesanan yang dilakukan oleh Joko juga bernilai 0.5 atau 50%.\n",
    "\n",
    "##### Likelihood: $P(X|y)$\n",
    "\n",
    "Terminologi kedua yang akan kita pelajari adalah \"Likelihood\". Dalam statistika sebenarnya ada sedikit perbedaan antara probability dan likelihood, hanya saja untuk menyederhanakan penjelasan pada pembelajaran kali ini, kita asumsikan bahwa keduanya sama. Likelihood untuk konsep Naive Bayes, bisa didefinisikan sebagai probability kemunculan nilai feature tertentu bila diketahui kemunculan nilai target lainnya.\n",
    "\n",
    "+ Referensi: https://en.wikipedia.org/wiki/Likelihood_function\n",
    "+ Asep: $\\begin{aligned} P(lumpia,bakso|Asep) &= (0.1 \\times 0.8) = 0.08\\end{aligned}$\n",
    "+ Joko: $\\begin{aligned} P(lumpia,bakso|Joko) &= (0.3\\times 0.2 = 0.06\\end{aligned}$\n",
    "\n",
    "##### Evidence atau Normalizer: $P(X)$\n",
    "\n",
    "Terminologi ketiga yang akan kita pelajari adalah \"Evidence atau Normalizer\". Evidence untuk konteks Naive Bayes, bisa didefinisikan sebagai total akumulasi dari hasil perkalian antara nilai likelihood dengan priornya seperti pada rumus dibawah ini.\n",
    "\n",
    "+ $Evidence = \\sum (Likelihood \\times Prior)$\n",
    "\n",
    "Untuk kasus kita kali ini adalah sebagai berikut.\n",
    "\n",
    "+ $P(lumpia,bakso) = (0.08 \\times 0.5)+ (0.06 \\times 0.5) = 0.07$\n",
    "\n",
    "##### Posterior Probability: $P(y|X)$\n",
    "\n",
    "Terminologi keempat yang akan kita pelajari terkait Naive Bayes adalah \"Posterior Probability\".\n",
    "\n",
    "+ Referensi: https://en.wikipedia.org/wiki/Posterior_probability\n",
    "+ Formula:\n",
    "$Posterior = \\frac{Likelihood \\times Prior}{Evidence}$\n",
    "+ Asep: $$\\begin{aligned} P(Asep|lumpia,bakso) &= \\frac{0.08 \\times 0.5}{0.7} = 0.57\\end{aligned}$$\n",
    "+ Joko: $$\\begin{aligned} P(Joko|lumpia,bakso) &= \\frac{0.06 \\times 0.5}{0.7} = 0.43\\end{aligned}$$\n",
    "\n",
    "Berdasarkan kalkulasi atau perhitungan diatas, kita bisa melihat bahwa nilai posterior Asep lebih tinggi dari nilia posterior Joko, maka \"Naive Bayes\" akan mengklasifikasikan aset sebagai pemesanannya.\n",
    "##### Studi Kasus 2\n",
    "\n",
    "Untuk memperjelas pemahaman, kita akan mempelajari studi kasus yang kedua. Disini kita diminta untuk melakukan klasifikasi pelanggan berdasarkan informasi pesanan yang diterima. Hanya saja, kali ini pesanannya adalah \"siomay\" dan \"bakso\".\n",
    "\n",
    "![](./images/asep_joko_snack.png)\n",
    "\n",
    "**Misi** :  Lakukan prediksi siapa pelanggan yang melakukan pemesanan dengan diketahui pesanannya adalah **siomay** dan **bakso**\n",
    "\n",
    "##### Posterior Probability: $P(y|X)$ (kasus 2)\n",
    "\n",
    "+ pesanan: siomay, bakso\n",
    "+ Evidence: $P(X)$\n",
    "\n",
    "Pertama-tama kita akan menghitung terlebih dahulu nilai \"Evindence\" atau \"Normalizer\" nya. Pada kasus kita kali ini, nilai evidence nya adalah probability dari pesanan siomay dan bakso. Dibawah ini adalah hasil perhitungan kalkulasinya.\n",
    "\n",
    "$P(siomay, bakso) = (0.1 \\times 0.8 \\times 0.5) + (0.5 \\times 0.2 \\times 0.5) = 0.09$\n",
    "\n",
    "Selanjutnya kita akan menghitung nilai probability Asep sebagai pemesan bila diketahui pesanannya adalah siomay dan bakso. Dibawah ini adalah hasil perhitungan kalkulasinya.\n",
    "\n",
    "+ Asep: $$\\begin{aligned} P(Asep|siomay,bakso) = \\frac{(0.1 \\times 0.8) \\times 0.5}{0.09} = 0.444\\end{aligned}$$\n",
    "\n",
    "Selanjutnya kita akan menghitung nilai probability Joko sebagai pemesan bila diketahui pesanannya adalah siomay dan bakso. Dibawah ini adalah hasil perhitungan kalkulasinya.\n",
    "\n",
    "+ Joko: $$\\begin{aligned} P(Joko|siomay,bakso) = \\frac{(0.5 \\times 0.2) \\times 0.5}{0.09} = 0.555\\end{aligned}$$\n",
    "\n",
    "## Mengapa disebut Naive?\n",
    "\n",
    "Mungkin ada banyak pertanyaan tentang mengapa model ini disebut Naive Bayes? Kata Bayes berasal dari nama seorang statistition yaitu Thomas Bayes. beliau lah yang menggagas suatu theorem yang akhirnya dikenal sebagai \"Bayes Theorem\". Lalu pertanyaan berikutnya adalah **Mengapa model ini disebut Naive?** dan jawaban singkatnya adalah sebagai berikut :\n",
    "\n",
    "+ Karena sewaktu kita mendefinisikan Likelihood $P(lumpia, bakso|Asep),$\n",
    "+ kita mengasumsikan $P(lumpia|Asep)$ conditionally independent terhadap $P(bakso|Asep);$ demikian sebaliknya.\n",
    "+ Sehingga dapat diformulasikan sebagai berikut:\n",
    "\n",
    "$P(lumpia, bakso|Asep) = P(lumpia|Asep) \\times P(bakso|Asep)$\n",
    "\n",
    "Disini Naive Bayes mengasumsikan bahwa pemesanan lumpia oleh Asep itu tidak dipengaruhi dan tidak ada hubungannya dengan pemesanan bakso oleh Asep. Padahal dalam kasus sebenarnya, bisa jadi pemesanan lumpia oleh Asep dipengaruhi oleh pemesanan bakso oleh Asep.\n",
    "\n",
    "## Dataset: Breast Cancer Wisconsin (Diagnostic)\n",
    "\n",
    "Selanjutnya kita akan mempelajari tahapan untuk menerapkan \"Naive Bayes\" model tersebut dengan SkLearn. Pertama kita akan siapkan terlebih dahulu datasetnya. Untuk kasus kali ini kita akan mengadopsi \"Breast Cancer Wisconsin\", ini merupakan open dataset yang ditawarkan oleh UCI machine learning.\n",
    "\n",
    "Referensi: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "\n",
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "print(load_breast_cancer().DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caranya:\n",
    "\n",
    "+ Pertama-tama kita akan mengimport terlebih dahulu modulnya dengan memanggil method \"from sklearn.datasets import load_breast_cancer\".\n",
    "+ Lalu selajutnya kita akan memanggil \"load_breast_cancer()\" dan disini kita akan coba akses description dari datasetnya dengan memanggil \".DESCR\".\n",
    "+ Lalu terakhir kita akan coba tampilkan pada layar dengan memanfaatkan fungsi print.\n",
    "\n",
    "Berdasarkan hasil diatas, kita bisa melihat isi deskripsi dari dataset \"Breast Cancer Wisconsin (Diagnostic)\", kita bisa ketahui juga bahwa dataset ini terdiri dari 569 instances atau 569 data, kita juga bisa lihat number of attributes adalah 30 numeric yang artinya disini terdapat 30 nilai numerik sebagai featurenya atau sebagai kolomnya yang kita bisa lihat featurenya terdiri dari :\n",
    "\n",
    "    - radius (mean of distances from center to points on the perimeter)\n",
    "    - texture (standard deviation of gray-scale values)\n",
    "    - perimeter\n",
    "    - area\n",
    "    - smoothness (local variation in radius lengths)\n",
    "    - compactness (perimeter^2 / area - 1.0)\n",
    "    - concavity (severity of concave portions of the contour)\n",
    "    - concave points (number of concave portions of the contour)\n",
    "    - symmetry\n",
    "    - fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "Semua feature tersebut memiliki type data numeric dengan total jumlah feature adalah 30. Terkait dengan class, terdapat 2 buah class pada dataset ini yaitu WDBC-Malignant (mengindikasikan tumor ganas) dan WDBC-Benign (mengindikasikan tumor jinak). Kita juga bisa melihat bahwa pada dataset ini tidak terdapat missing value. Lalu terkait proporsi classnya, terdapat 212 data yang diklasifikasikan sebagai malignant atau tumor ganas dan terdapat 357 data yang diklasifikasikan sebagai benign atau tumor jinak. Lalu selanjutnya ada keterangan creator atau pembuat dataset, dan ada juga tanggal pembuatannya. Kita sebaiknya mengetahui deskripsi suatu dataset terlebih dahulu sebelum bekerja dengan dataset yang akan digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_breast_cancer?\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caranya:\n",
    "\n",
    "+ Selanjutnya kita akan melakukan akses pada dataset tersebut.\n",
    "+ Jika kita bingung terhadap suatu dataset, jupyter notebook bisa memberikan informasi tentang dataset yang akan kita gunakan dengan memanggil fungsi \"load_nama_database?\" yang dalam kasus ini berarti \"load_breast_cancer?\".\n",
    "+ Disini kita menemukan salah satu parameter yang bisa kita sertakan pada function \"load_breast_cancer\" yaitu \"return_X_y\" yang dalam kasus kita kali ini, kita akan set sebagai true yang artinya function ini akan secara otomatis membagi dan memisahkan antar features data dengan label datanya dan tentunya label semacam ini akan membantu dalam memudahkan pekerjaan yang kita lakukan.\n",
    "+ Pada code diatas, kita akan coba eksekusi code nya \"X, y = load_breast_cancer\" yang kita beri nilai parameternya \"return_X_y=True\". X dan y perlu kita sertakan untuk menangkap nilai kembalian dari function tersebut, dimana nilai X digunakan untuk menampung nilai feature dan y digunakan untuk menampung nilai targetnya.\n",
    "+ Lalu kita panggil \"X.shape\" untuk melihat dimensinya, yang bisa kita lihat bahwa jumlah barisnya ada 569 dan kolomnya berjumlah 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Testing Dataset\n",
    "\n",
    "Selanjutnya kita akan bagi dataset ini kedalam training dan testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (455, 30)\n",
      "X_test shape (114, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "print(f'X_train shape {X_train.shape}')\n",
    "print(f'X_test shape {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caranya:\n",
    "\n",
    "+ Pertama-tama kita akan mencoba import terlebih dahulu modulnya dengan memanggil fungsi \"from sklearn.model_selection import train_test_split\".\n",
    "+ Selanjutnya kita akan memanggil \"train_test_split\" dengan menyertakan parameter \"(X, y, test_size=0.2 (artinya 20% dari data kita akan dialokasikan untuk testing dataset, sedangkan 80% akan digunakan sebagai training dataset), random_state=0). Fungsi ini akan mengembalikan 4 buah kumpulan nilai yang perlu kita tangkap kedalam 4 variabel yaitu \"X_train, X_test, y_train, dan y_test\".\n",
    "+ Untuk selanjutnya, kita akan coba untuk menampilkan dimensi dari X_train dan X_test nya.\n",
    "\n",
    "Bisa kita lihat pada code diatas, untuk X_train jumlah datanya ada 455 dan untuk X_test ada 114 artinya X_train merupakan 80% dan X_test 20% dari keseluruhan dataset yang kita miliki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes dengan Scikit Learn\n",
    "\n",
    "Pada kali ini kita akan menerapkan \"Naive Bayes\" untuk melakukan klasifikasi data pada dataset \"Breast Cancer\" yang kita miliki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caranya:\n",
    "\n",
    "+ Pertama-tama kita mengimport terlebih dahulu modelnya dengan memanggil fungsi \"from sklearn.naive_bayes import GaussianNB\". NB merepresentasikan Naive Bayes.\n",
    "+ Lalu kita juga akan mengimport matriks akurasinya dengan memanggi fungsi \"from sklearn.metrics import accuracy_score\".\n",
    "+ Berikutnya kita akan membentuk terlebih dahulu objek dari model \"GaussianNB()\" yang ditampung kedalam variabel \"model\".\n",
    "+ Lalu selanjutnya, model ini akan training dengan menggunakan method \"fit()\" dengan menyertakan parameter \"(X_train, y_train)\".\n",
    "+ Setelah modelnya kita training, maka train model tersebut akan kita gunakan untuk melakukan prediksi terhadap nilai dari X_test yang hasil prediksinya akan kita tampung kedalam variabel \"y_pred\".\n",
    "+ Untuk selanjutnya, kita akan mengukur nilai akurasinya dengan memanggil fungsi \"accuracy_score\" dengan menyertakan parameter \"(y_test, y_pred)\".\n",
    "\n",
    "Kita bisa lihat bahwa hasil akurasinya adalah 0.9298245614035088. Kita juga bisa mencari nilai akurasinya dengan cara yang lebih sederhana (perhatikan pada code selanjutnya)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pemanggilan pada code diatas tersebut setara dengan pemanggilan fungsi baris code ke 5 dan 6 pada code sebelumnya."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
